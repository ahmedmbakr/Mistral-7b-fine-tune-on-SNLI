{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial notebook that I worked from: https://kaitchup.substack.com/p/mistral-7b-recipes-for-fine-tuning\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "num_training_epochs = 1 \n",
    "num_eval_steps = 250 \n",
    "sequence_length = 256\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "gradient_accumulation_steps = 4 \n",
    "gpu_id = 0 \n",
    "num_output_labels = 4 \n",
    "hugging_face_token = \"TODO: Add here your hugging face token\" # Hugging Face API token\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer_padding_side = \"right\"\n",
    "models_output_dir = \"./v2_mistral7b_results\"\n",
    "start_from_checkpoint_with_name = None # This can be a string with the checkpoint name to start from, or None to start from scratch.\n",
    "print_dataset_statistics_flag = False # If True, the statistics of the dataset will be printed.\n",
    "skip_training = True # If True, the training will be skipped. I added this option because sometime I want only to evaluate the model.\n",
    "\n",
    "DEBUG_MODE = True\n",
    "# For debugging purposes\n",
    "if DEBUG_MODE:\n",
    "    num_training_examples = 1000 # Number of training examples to use [0: num_training_examples]\n",
    "    num_validation_examples = 100 # Number of validation examples to use [0: num_validation_examples]\n",
    "    num_test_examples = 100  # Number of test examples to use [0: num_test_examples]\n",
    "\n",
    "if start_from_checkpoint_with_name is not None:\n",
    "    model_name = models_output_dir + \"/\" + start_from_checkpoint_with_name\n",
    "    print(\"Starting from checkpoint with name: \", start_from_checkpoint_with_name)\n",
    "\n",
    "print(\"Model name: \", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_value = 42\n",
    "\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_and_label(example):\n",
    "    if example[\"label\"] == -1: # This is a special value (from the dataset) that means the example there is no valid label for this example\n",
    "        example[\"label\"] = 3\n",
    "    return {\n",
    "        \"text\": f'Prem: {example[\"premise\"]} Hypo: {example[\"hypothesis\"]}', # Premise and hypothesis concatenated\n",
    "        \"labels\": example.pop(\"label\")  \n",
    "    }\n",
    "\n",
    "def create_tokenize_function(tokenizer):\n",
    "    def tokenize_function(example):\n",
    "        tokenized = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=sequence_length)\n",
    "        return tokenized\n",
    "    return tokenize_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer import ConstantLengthDataset\n",
    "from huggingface_hub import login\n",
    "login(token=hugging_face_token) # I created this token on huggingface.co/settings/tokens\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = tokenizer_padding_side\n",
    "\n",
    "# Dataset source: https://huggingface.co/datasets/stanfordnlp/snli\n",
    "dataset = load_dataset(\"stanfordnlp/snli\")\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    dataset[\"train\"] = dataset[\"train\"].select(range(num_training_examples))\n",
    "    dataset[\"validation\"] = dataset[\"validation\"].select(range(num_validation_examples))\n",
    "    dataset[\"test\"] = dataset[\"test\"].select(range(num_test_examples))\n",
    "\n",
    "dataset = dataset.map(concatenate_and_label)\n",
    "tokenize_function = create_tokenize_function(tokenizer)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(\"data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Counter the number of examples for each label in the training set\n",
    "print(\"The number of examples for each label in the training set: \", Counter(tokenized_dataset[\"train\"][\"labels\"]))\n",
    "# They use -1 to label examples for which gold label is missing (gold label = - in the original dataset). Based on this link: https://github.com/huggingface/datasets/issues/296\n",
    "# print(\"The indices of the examples in the training set that have the label -1 (no label is provided): \", tokenized_dataset[\"train\"].filter(lambda example: example[\"labels\"] == -1)[\"idx\"])\n",
    "\n",
    "# Print the first example\n",
    "# Retrieve the first example from the tokenized dataset\n",
    "first_example = tokenized_dataset['train'][0]  # Assuming you want to visualize from the training split\n",
    "\n",
    "# Decode the tokenized input back to text\n",
    "decoded_text = tokenizer.decode(first_example['input_ids'], skip_special_tokens=True)\n",
    "decoded_text_with_special_tokens = tokenizer.decode(first_example['input_ids'], skip_special_tokens=False)\n",
    "\n",
    "# Print the decoded text and the corresponding label\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "print(\"Decoded Text with Special Tokens:\", decoded_text_with_special_tokens)\n",
    "print(\"Label:\", first_example['labels']) \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Extract input_ids, attention_mask, and labels from the batch\n",
    "    input_ids = [torch.tensor(example[\"input_ids\"]).to(f'cuda:{gpu_id}') for example in batch]\n",
    "    attention_mask = [torch.tensor(example[\"attention_mask\"]).to(f'cuda:{gpu_id}') for example in batch]\n",
    "    labels = [torch.tensor(example[\"labels\"]).to(f'cuda:{gpu_id}') for example in batch]\n",
    "\n",
    "    input_ids_padded = torch.stack(input_ids)\n",
    "    attention_mask_padded = torch.stack(attention_mask)\n",
    "\n",
    "    # Stack the labels into a single tensor\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\"input_ids\": input_ids_padded, \"attention_mask\": attention_mask_padded, \"labels\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=batch_size, shuffle=False) \n",
    "# Get the first batch of the training dataloader\n",
    "first_batch = next(iter(train_dataloader))\n",
    "\n",
    "print(\"First batch input ids: \", first_batch['input_ids'].shape)\n",
    "print(\"First batch labels: \", first_batch['labels'])\n",
    "decoded_text_for_batch = [tokenizer.decode(input_ids, skip_special_tokens=True) for input_ids in first_batch['input_ids']]\n",
    "print(\"First batch decoded text: \", decoded_text_for_batch)\n",
    "print(\"Finished debugging the batch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_statistics(set_name):\n",
    "    max_length_premise = max(len(tokenizer(example[\"premise\"])[\"input_ids\"]) for example in dataset[set_name])\n",
    "    max_length_hypothesis = max(len(tokenizer(example[\"hypothesis\"])[\"input_ids\"]) for example in dataset[set_name])\n",
    "    print(f\"Statistics for the {set_name} dataset:\")\n",
    "    print(f\"\\tNumber of examples: {len(dataset[set_name])}\")\n",
    "    print(f\"\\tMaximum length of the premise: {max_length_premise}, Maximum length of the hypothesis: {max_length_hypothesis}. Therefore, the maximum length of the concatenated text is {max_length_premise + max_length_hypothesis}\")\n",
    "\n",
    "if print_dataset_statistics_flag:\n",
    "    print_dataset_statistics(\"train\")\n",
    "    print_dataset_statistics(\"validation\")\n",
    "    print_dataset_statistics(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantization configuration\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "          model_name, quantization_config=bnb_config, num_labels=num_output_labels, device_map={\"\": gpu_id}\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the configuration of LoRA.\n",
    "from peft.utils import TaskType\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ")\n",
    "\n",
    "# Training hyperparameters\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=models_output_dir,\n",
    "        eval_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        do_predict=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        log_level=\"debug\",\n",
    "        learning_rate=learning_rate,\n",
    "        eval_steps=num_eval_steps,\n",
    "        num_train_epochs=num_training_epochs,\n",
    "        save_steps=num_eval_steps,\n",
    "        warmup_steps=num_eval_steps,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")\n",
    "\n",
    "# print the trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "print_trainable_parameters(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomSFTTrainer(SFTTrainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn \n",
    "        )\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=self.args.per_device_eval_batch_size,\n",
    "            collate_fn=collate_fn \n",
    "        )\n",
    "    \n",
    "    def get_test_dataloader(self, test_dataset):\n",
    "        return DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.args.per_device_eval_batch_size,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "trainer = CustomSFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=tokenized_dataset['train'],\n",
    "            eval_dataset=tokenized_dataset['validation'],\n",
    "            peft_config=peft_config,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=sequence_length,\n",
    "            tokenizer=tokenizer,\n",
    "            args=training_arguments,\n",
    "    )\n",
    "if not skip_training: \n",
    "    if start_from_checkpoint_with_name is not None:\n",
    "        print(\"Starting the training from the checkpoint with name: \", start_from_checkpoint_with_name)\n",
    "        trainer.train(resume_from_checkpoint=model_name)\n",
    "    else:\n",
    "        print(\"Starting the training from original Mistral 7B model weights.\")\n",
    "        trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model.config.use_cache = True\n",
    "model = PeftModel.from_pretrained(model, \"./v2_mistral7b_results/checkpoint-2250/\") # This model affects this cell and the next cell that calculates the validation accuracy on the test dataset.\n",
    "\n",
    "model.cuda(gpu_id) \n",
    "model.eval() \n",
    "\n",
    "def classify(prem, hypo):\n",
    "    # Create the prompt as it was used during training\n",
    "    prompt = f'Prem: {prem} Hypo: {hypo}'\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=sequence_length)\n",
    "    input_ids_list = inputs[\"input_ids\"].tolist()[0]\n",
    "    decoded_text_with_special_tokens = tokenizer.decode(input_ids_list, skip_special_tokens=False)\n",
    "    print(\"Decoded Text with Special Tokens:\", decoded_text_with_special_tokens)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].cuda(gpu_id) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "    print(\"For the prompt: \", prompt)\n",
    "\n",
    "    probabilities = (probabilities * 100).tolist()\n",
    "    print(\"Probabilities: \", probabilities)\n",
    "\n",
    "    classes_dict = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\", 3: \"no valid label\"}\n",
    "    print(f\"Predicted class: {classes_dict[predicted_class]}\")\n",
    "\n",
    "# Test the function with example premises and hypotheses\n",
    "classify(\"The moon's gravity affects the Earth.\", \"The Earth revolves around the sun.\") # Should be entailment\n",
    "classify(\"The Earth revolves around the Sun in a circular orbit.\", \"The Earth revolves around the Sun in an elliptical orbit.\") # Should be contradiction\n",
    "classify(\"The football player scored a goal.\", \"The football player is a striker.\") # Should be neutral\n",
    "classify(\"The football player scored a goal.\", \"The football player is a goalkeeper.\") # Should be contradiction\n",
    "classify(\"The football player scored a goal.\", \"The football player is a defender.\") # Should be contradiction\n",
    "classify(\"A person is outside on a beautiful day.\", \"The weather is nice.\") # Should be entailment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = trainer.predict(test_dataset=tokenized_dataset['test'])\n",
    "predictions_tensor = torch.tensor(predictions_dict.predictions)\n",
    "predictions = torch.argmax(predictions_tensor, dim=-1)\n",
    "predictions = predictions.cpu().numpy()\n",
    "print(\"Prediction metrics: \", predictions_dict.metrics)\n",
    "accuracy = (predictions == predictions_dict.label_ids).mean()\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
